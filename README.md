# Quant Starter ğŸ“Š

A lightweight, standardized Python project skeleton designed for quantitative finance analysis.

This project provides foundational tools for:
- **Data I/O**: CSV and Parquet file handling
- **Data Quality**: Detecting missing values, logical errors, outliers
- **Data Processing**: ETL pipeline with cleaning and feature engineering
- **Quantitative Metrics**: Mean, Volatility, Max Drawdown

---

## ğŸ“ Project Structure

```text
quant_starter/
â”œâ”€â”€ README.md                   # Project documentation
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .gitignore                  # Git ignore rules
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Raw data files
â”‚   â”‚   â”œâ”€â”€ sample_prices.csv
â”‚   â”‚   â”œâ”€â”€ dirty_stock_data.csv
â”‚   â”‚   â”œâ”€â”€ clean_stock_data.csv
â”‚   â”‚   â””â”€â”€ stock_data_dirty.csv
â”‚   â””â”€â”€ processed/              # Processed data (generated)
â”‚       â””â”€â”€ market_data.parquet
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py               # Configuration settings
â”‚   â”œâ”€â”€ io_utils.py             # File I/O utilities
â”‚   â”œâ”€â”€ metrics.py              # Quantitative metrics (Mean, Std, MaxDD)
â”‚   â”œâ”€â”€ data_checker.py         # Data quality inspector
â”‚   â””â”€â”€ processors.py           # ETL: Cleaning & Feature Engineering
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_metrics.py         # Tests for metrics
â”‚   â”œâ”€â”€ test_checker.py         # Tests for data checker
â”‚   â””â”€â”€ test_processors.py      # Tests for data processor
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ run_basic_report.py     # Basic analysis report
    â”œâ”€â”€ check_my_data.py        # Data quality check script
    â””â”€â”€ run_etl.py              # ETL pipeline runner
    
    
ğŸš€ Getting Started

1. Prerequisites

Python 3.8 or higher
pip (Python package manager)

2. Installation

# Clone the repository
git clone <repository-url>
cd quant_starter

# Install dependencies
pip install -r requirements.txt

3. Verify Installation

# Run all tests
pytest tests/ -v

ğŸ“Š Module 1: Basic Metrics
Calculate fundamental quantitative metrics.

Usage
python scripts/run_basic_report.py

Features
Function	Description	Complexity
mean(x)	Arithmetic mean	O(N)
std(x)	Standard deviation (population)	O(N)
max_drawdown(nav)	Maximum drawdown from NAV series	O(N)
Max Drawdown Algorithm
Efficient single-pass O(N) implementation:
def max_drawdown(nav_series):
    max_dd = 0.0
    peak = nav_series[0]
    
    for nav in nav_series:
        if nav > peak:
            peak = nav
        drawdown = (peak - nav) / peak
        if drawdown > max_dd:
            max_dd = drawdown
    
    return max_dd
    
ğŸ” Module 2: Data Quality Checker
Detect common data quality issues in OHLCV financial data.

Usage

# Check default test file
python scripts/check_my_data.py

# Check a specific file
python scripts/check_my_data.py path/to/your/data.csv

Checks Performed
Category	Check	Description
Integrity	Missing Values	Detects NaN/NULL values
Integrity	Duplicate Dates	Finds repeated date entries
Logic	High < Low	Impossible price relationship
Logic	Price Range	Open/Close within [Low, High]
Logic	Negative Values	Prices/volume cannot be negative
Continuity	Date Gaps	Gaps larger than 3 days
Outliers	Extreme Moves	Daily returns exceeding Â±10%

Sample Output
============================================================
        ğŸ“‹ DATA QUALITY REPORT
============================================================

ğŸ“Š BASIC INFO
----------------------------------------
   Total Rows: 20

ğŸ” MISSING VALUES
----------------------------------------
   open: 1 missing
   close: 1 missing

âš ï¸  LOGICAL CONSISTENCY
----------------------------------------
   High < Low errors:     1
   Total logical errors:  4

============================================================
âŒ RESULT: 12 issue(s) found. Please review.
============================================================

ğŸ­ Module 3: Data Processor (ETL Pipeline)
Clean raw data and generate technical features for strategy research.

Usage
# Run with default paths
python scripts/run_etl.py

# Specify custom paths
python scripts/run_etl.py --input data/raw/my_data.csv --output data/processed/output.parquet

# Run with verification
python scripts/run_etl.py --verify

ETL Pipeline Flow

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXTRACT                                                    â”‚
â”‚  â””â”€â”€ Read CSV file                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  TRANSFORM                                                  â”‚
â”‚  â”œâ”€â”€ Clean: Forward Fill (ffill) â†’ Backward Fill (bfill)    â”‚
â”‚  â”œâ”€â”€ Clean: Remove duplicate dates                          â”‚
â”‚  â”œâ”€â”€ Feature: Daily Return                                  â”‚
â”‚  â”œâ”€â”€ Feature: MA5 (5-day Moving Average)                    â”‚
â”‚  â”œâ”€â”€ Feature: MA20 (20-day Moving Average)                  â”‚
â”‚  â””â”€â”€ Feature: Vol_20 (20-day Rolling Volatility)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  LOAD                                                       â”‚
â”‚  â””â”€â”€ Save to Parquet format                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Technical Features Added
Feature	Description	Formula
daily_return	Daily percentage change	(close[t] - close[t-1]) / close[t-1]
MA5	5-day Simple Moving Average	mean(close[t-4:t+1])
MA20	20-day Simple Moving Average	mean(close[t-19:t+1])
Vol_20	20-day Rolling Volatility	std(daily_return[t-19:t+1])

Data Cleaning Strategy
Forward Fill (ffill) + Backward Fill (bfill)

Original:           After ffill:        After bfill:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Row 0: NaN          Row 0: NaN          Row 0: 100  â† bfill
Row 1: 100          Row 1: 100          Row 1: 100
Row 2: NaN    â†’     Row 2: 100    â†’     Row 2: 100  â† ffill
Row 3: 105          Row 3: 105          Row 3: 105

Programmatic Usage
import pandas as pd
from src.processors import DataProcessor

# Load raw data
df = pd.read_csv('data/raw/stock_data.csv')

# Create processor and run pipeline
processor = DataProcessor(df)
processor.clean()           # Handle NaN, duplicates
processor.add_features()    # Add MA5, MA20, Vol_20

# Save to Parquet
processor.save_to_parquet('data/processed/output.parquet')

# Or use method chaining
processor = DataProcessor(df)
processor.clean().add_features().save_to_parquet('output.parquet')

Sample Output
============================================================
        ğŸ­ ETL PIPELINE
============================================================

ğŸ“¥ STEP 1: EXTRACT
----------------------------------------
   Reading: data/raw/stock_data_dirty.csv
   âœ… Loaded 25 rows, 6 columns
   Missing values: 4

ğŸ§¹ STEP 2: TRANSFORM (Cleaning)
----------------------------------------
   âœ… Missing values after cleaning: 0
   âœ… Duplicate dates removed

âš™ï¸  STEP 3: TRANSFORM (Feature Engineering)
----------------------------------------
   âœ… New features added: ['daily_return', 'MA5', 'MA20', 'Vol_20']
   MA5:  21 valid values (first 4 are NaN)
   MA20: 6 valid values (first 19 are NaN)

ğŸ’¾ STEP 4: LOAD
----------------------------------------
âœ… Data saved to: data/processed/market_data.parquet
   Rows: 25, Columns: 10

============================================================
âœ… ETL Pipeline completed successfully!
============================================================


Important Notes
âš ï¸ No Look-ahead Bias
All calculations use only past data. The rolling() function includes:

Current row
Previous (N-1) rows

# âœ… Correct: MA5 uses rows [t-4, t-3, t-2, t-1, t]
df['MA5'] = df['close'].rolling(window=5).mean()

# âŒ Wrong: Using future data would cause look-ahead bias
# df['MA5'] = df['close'].shift(-2).rolling(window=5).mean()

NaN Values in Features
Feature	First N rows are NaN	Reason
daily_return	1	No previous day to compare
MA5	4	Need 5 days of data
MA20	19	Need 20 days of data
Vol_20	20	Need 20 returns (21 prices)

ğŸ§ª Testing
Run All Tests
pytest tests/ -v

Run Specific Test File
# Test metrics module
pytest tests/test_metrics.py -v

# Test data checker module
pytest tests/test_checker.py -v

# Test data processor module
pytest tests/test_processors.py -v

Test Coverage Summary
Module	Tests	Coverage
metrics.py	14	mean, std, max_drawdown
data_checker.py	18	All quality checks
processors.py	22	Cleaning, features, file I/O
Total	54

ğŸ“ Data Formats
Input: OHLCV CSV

date,open,high,low,close,volume
2024-01-01,100.00,105.00,99.00,103.00,1000000
2024-01-02,103.00,108.00,102.00,106.00,1100000

Output: Processed Parquet
Column	Type	Description
date	datetime	Trading date
open	float	Opening price
high	float	Highest price
low	float	Lowest price
close	float	Closing price
volume	int	Trading volume
daily_return	float	Daily return (%)
MA5	float	5-day moving average
MA20	float	20-day moving average
Vol_20	float	20-day volatility

âš ï¸ Important Notes
Vectorized Operations
All data processing uses Pandas vectorized operations for performance:

# âœ… Correct (fast) - Vectorized
df['MA5'] = df['close'].rolling(window=5).mean()

# âŒ Wrong (slow) - Row iteration
for i in range(len(df)):
    df.loc[i, 'MA5'] = df['close'].iloc[max(0,i-4):i+1].mean()
    
Parquet vs CSV
Aspect	CSV	Parquet
Read Speed	Slow	10-100x faster
File Size	Large	Compressed
Type Preservation	No	Yes
Human Readable	Yes	No

ğŸ› ï¸ Development Log
Version	Task	Description
v0.1	L0-Task1	Project skeleton, metrics, basic report
v0.2	L0-Task2	Data quality checker with tests
v0.3	L1-Task1	ETL pipeline: cleaning & feature engineering


ğŸ“„ License
MIT License


---

## 5.3 æäº¤åˆ° Git

```bash
git add .
git commit -m "Update README with L1-Task1 ETL pipeline documentation"

5.4 æŸ¥çœ‹ Git æäº¤å†å²
git log --oneline
ä½ åº”è¯¥çœ‹åˆ°å¤šæ¬¡æäº¤è®°å½•ã€‚

5.5 æœ€ç»ˆéªŒæ”¶æ£€æŸ¥
# 1. è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest tests/ -v

# 2. è¿è¡Œ ETL è„šæœ¬
python scripts/run_etl.py --verify

# 3. æ£€æŸ¥ç”Ÿæˆçš„ Parquet æ–‡ä»¶
python -c "import pandas as pd; df = pd.read_parquet('data/processed/market_data.parquet'); print(df.head())"


